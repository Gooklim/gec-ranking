#!/usr/bin/env python

# Courtney Napoles
# <courtneyn@jhu.edu>
# 21 June 2015
# ##
# compute_gleu
# 
# This script calls gleu.py to calculate the GLEU score of a sentence, as
# described in our ACL 2015 paper, Ground Truth for Grammatical Error 
# Correction Metrics by Courtney Napoles, Keisuke Sakaguchi, Matt Post, 
# and Joel Tetreault.
# 
# For instructions on how to get the GLEU score, call "compute_gleu -h"
#
# This script was adapted from compute-bleu by Adam Lopez.
# <https://github.com/alopez/en600.468/blob/master/reranker/>

import argparse
import sys
import os
from gleu import GLEU
import scipy.stats
import numpy as np
import random

def print_gleu_stats(scores) :
    mean = np.mean(scores)
    std = np.std(scores)
    ci = scipy.stats.norm.interval(0.95,loc=mean,scale=std)
    print 'Mean=',mean,'Std=',std,'95%CI=','(%.3f,%.3f)'%(ci[0],ci[1])

if __name__ == '__main__' :
    
    parser = argparse.ArgumentParser()
    parser.add_argument("-r", "--reference", 
                        help="Target language reference sentences. Multiple "
                        "files for multiple references.",
                        nargs="*",
                        dest="reference",
                        required=True) 
    parser.add_argument("-s", "--source",
                        help="Source language source sentences",
                        dest="source", 
                        required=True) 
    parser.add_argument("-o", "--hypothesis", 
                        help="Target language hypothesis sentences to evaluate "
                        "(can be more than one file--the GLEU score of each "
                        "file will be output separately). Use '-o -' to read "
                        "hypotheses from stdin.",
                        nargs="*", 
                        dest="hypothesis", 
                        required=True)
    parser.add_argument("-n",
                        help="Maximum order of ngrams",
                        type=int,
                        default=4)
    parser.add_argument("-d","--debug",
                        help="Debug; print sentence-level scores",
                        default=False,
                        action="store_true")
    parser.add_argument('-v','--version',
                        type=int,
                        default=1,
                        help='which version of GLEU to use.'
                        '\t1: (R^H-S\R^H)/(R^H) randomly sampled 500 times, '
                        'using one randomly selected reference each time\n'
                        '\t2: (R^H-S\R_w^H)/(R^H), where R_w is the normalized '
                        'count\n')
    parser.add_argument('--iter',
                        type=int,
                        default=500,
                        help='the number of iterations to run')
    parser.add_argument('--printiter',
                        default=False,
                        action='store_true',
                        help='print detailed information about each iteration')
    
    args = parser.parse_args()

    num_iterations = args.iter if args.version == 1 else 1

    # if there is only one reference, just do one iteration
    if len(args.reference) == 1 :
        num_iterations = 1
    
    gleu_calculator = GLEU(args.n)

    gleu_calculator.load_sources(args.source)
    gleu_calculator.load_references(args.reference)
    
    for hpath in args.hypothesis :
        instream = sys.stdin if hpath == '-' else open(hpath)
        hyp = [line.split() for line in instream]

        if not args.debug :
            print os.path.basename(hpath),

        if args.version == 1 :
            # first generate a random list of indices, using a different seed
            # for each iteration
            indices = []
            for j in range(num_iterations) :
                random.seed(j*101)
                indices.append([random.randint(0,len(args.reference)-1)
                                for i in range(len(hyp))])

            if args.debug :
                print
                print '===== Sentence-level scores ====='
                print 'SID GLEU'

            iter_stats = [ [0 for i in xrange(2*args.n+2)]
                           for j in range(num_iterations) ]

            for i,h in enumerate(hyp) :
                
                gleu_calculator.load_hypothesis_sentence(h)
                # we are going to store the score of this sentence for each ref
                # so we don't have to recalculate them 500 times
                
                stats_by_ref = [ None for r in range(len(args.reference)) ]
                                
                for j in range(num_iterations) :
                    ref = indices[j][i]
                    this_stats = stats_by_ref[ref]

                    if args.printiter :
                        ## print out which reference index was chosen
                        print >>sys.stderr,ref,

                    if this_stats is None :
                        this_stats = [ s for s in gleu_calculator.gleu_stats(
                            i,r_ind=ref,version=args.version) ]
                        stats_by_ref[ref] = this_stats
                        
                    iter_stats[j] = [ sum(scores)
                                      for scores in zip(iter_stats[j], this_stats)]
                if args.printiter :
                    print >>sys.stderr,''
                if args.debug :
                    # sentence-level GLEU is the mean GLEU of the hypothesis compared to
                    # each reference 
                    for r in range(len(args.reference)) :
                        if stats_by_ref[r] is None :
                            stats_by_ref[r] = [ s for s in gleu_calculator.gleu_stats(
                                i,r_ind=r,version=args.version) ]
                    print i,np.mean([gleu_calculator.gleu(stats) for stats in stats_by_ref])

            if args.debug :
                print '\n==== Overall score ====='

            if args.printiter :
                # print sufficient statistics and GLEU for each iteration
                for stats in iter_stats :
                    print >>sys.stderr,stats,gleu_calculator.gleu(stats)
            print_gleu_stats([gleu_calculator.gleu(stats)
                              for stats in iter_stats ])
                                
        if args.version == 2 :
            stats = [0 for i in xrange(2*args.n+2)]
            for i,h in enumerate(hyp):
                gleu_calculator.load_hypothesis_sentence(h)
                this_stats = [ s for s in gleu_calculator.gleu_stats(
                    i,version=args.version) ]
                if args.debug :
                    print i,gleu_calculator.gleu(this_stats)
                stats = [sum(scores) for scores in zip(stats, this_stats)]

            if args.debug :
                print '\n==== Overall score ====='
            gleu_score = gleu_calculator.gleu(stats)
            print >>sys.stderr,''
        
            print gleu_score
